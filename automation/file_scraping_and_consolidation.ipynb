{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac22bdce",
   "metadata": {},
   "source": [
    "## Automation - Scraping data publications from NHS England web pages and consolidating them into one file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5872600",
   "metadata": {},
   "source": [
    "This demonstration will show you how you can automatically download multiple published data files from different (related) web pages and consolidate them into one file. The example is of the **Learning Disability Health Check Scheme**\n",
    "\n",
    "[Data landing page](https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c4a0c",
   "metadata": {},
   "source": [
    "### Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3d203",
   "metadata": {},
   "source": [
    "### Using urljoin to construct URLs\n",
    "\n",
    "This will extract any .csv files for the calendar year 2024, which are all saved to their individual web pages, meaning that urljoin is required to construct the URLs dynamically (i.e. so that you don't have to hard code all the indidual web pages).\n",
    "\n",
    "The package \"re\" is imported so that regular expression logic can be used in the construction of the URLs i.e. anything matching the patterm of the regular expression will be considered a web page of interest. (NOTE: you do not need to install \"re\", it is native to Python)\n",
    "\n",
    "It's been limited to 2024 files to reduce the amount of data being transferred, but you could use a different regular expression to cover more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef96ab",
   "metadata": {},
   "source": [
    "### Re-usability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc3c915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .csv file: https://files.digital.nhs.uk/89/11D2B8/learning-disabilities-health-check-scheme-eng-Dec-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Dec-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/DB/E2BCB6/learning-disabilities-health-check-scheme-eng-Nov-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Nov-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/8E/429E64/learning-disabilities-health-check-scheme-eng-Oct-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Oct-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/1E/56812A/learning-disabilities-health-check-scheme-eng-Sep-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Sep-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/1C/BF3E28/learning-disabilities-health-check-scheme-eng-Aug-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Aug-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/0C/DC2F3D/learning-disabilities-health-check-scheme-eng-Jul-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Jul-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/BF/E282F0/learning-disabilities-health-check-scheme-eng-Jun-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Jun-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/71/2A817F/learning-disabilities-health-check-scheme-eng-May-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-May-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/A7/B01710/learning-disabilities-health-check-scheme-eng-Apr-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Apr-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/61/2C1997/learning-disabilities-health-check-scheme-eng-Mar-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Mar-2024.csv\n",
      "Found .csv file: https://files.digital.nhs.uk/DD/8677F7/learning-disabilities-health-check-scheme-eng-Feb-2024.csv\n",
      "Downloaded: learning-disabilities-health-check-scheme-eng-Feb-2024.csv\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "import requests as req\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme'\n",
    "\n",
    "target_urls = []                           # empty list that will later get filled with target URLs in a for loop.\n",
    "\n",
    "dynamic_section = r'^england-[a-z]+-2024$' # the regular expression for the URLs we are interested in. note that the $ implies that you don't want anything else to follow.\n",
    "\n",
    "response = req.get(url)                  # get the response from the base URL\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup5 = BeautifulSoup(response.content, \"html.parser\")     # if there is a successful response, create a BeautifulSoup object.\n",
    "\n",
    "    for link in soup5.find_all('a', href = True):\n",
    "        sublink = link[\"href\"]\n",
    "        if re.match(dynamic_section,sublink.split('/')[-1]):\n",
    "            full_url = urljoin(url, sublink)                   # for each of the instances of the pattern we are looking for\n",
    "            target_urls.append(full_url)                        # add the constructed full URL to a list of target URLs\n",
    "        \n",
    "    for link in target_urls:                                    # check for a successful response (code 200) from each URL\n",
    "        response = req.get(link)                                # and create a BeautifulSoup object for each.\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            for link in soup.find_all(\"a\", href=True):          # for each URL found on each of the pages in target_urls...\n",
    "                file_url = link['href']                         \n",
    "\n",
    "                if file_url.endswith(('.csv')):                 # ... check for .csv file extensions\n",
    "                    print(\"Found .csv file:\", file_url)\n",
    "\n",
    "                    file_name = file_url.split(\"/\")[-1]         # extract the file name from the URL i.e. everything after the last /\n",
    "                    file_response = req.get(file_url)           # check the response for each file\n",
    "\n",
    "                    file_path = os.path.join(\"downloads\",       # set up the file path for the downloaded files to the \"downloads\" folder.\n",
    "                                              file_name)\n",
    "            \n",
    "                    if file_response.status_code == 200:        # if there's a successful response\n",
    "                        \n",
    "                        with open(file_path, \"wb\") as file:     # save the file to the downloads directory\n",
    "                            file.write(file_response.content)\n",
    "                        print(f\"Downloaded: {file_name}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download: {file_url}\")\n",
    "\n",
    "else:\n",
    "    print(f'Failed to fetch webpage: {response.status_code}')   # this else statement pairs with the original response code check for the base URL\n",
    "                                                                # (see the first \"if\" in this code block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234ee72",
   "metadata": {},
   "source": [
    "### Consolidating the files into one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
